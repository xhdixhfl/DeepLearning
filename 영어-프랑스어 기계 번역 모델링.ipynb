{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/F+sSFEjqdeUDthpzh5jn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhdixhfl/last_project/blob/main/%EC%98%81%EC%96%B4-%ED%94%84%EB%9E%91%EC%8A%A4%EC%96%B4%20%EA%B8%B0%EA%B3%84%20%EB%B2%88%EC%97%AD%20%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 셋 로딩"
      ],
      "metadata": {
        "id": "XxXj3iC45gkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lagtGGuUHegL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a8efd1-06fe-48d1-dc27-020e6b196df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-06 01:24:47--  http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.79.128, 108.177.127.128, 172.217.218.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.79.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3423204 (3.3M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   3.26M  4.78MB/s    in 0.7s    \n",
            "\n",
            "2023-01-06 01:24:48 (4.78 MB/s) - ‘fra-eng.zip’ saved [3423204/3423204]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## 데이터 셋 로딩\n",
        "# https://www.manythings.org/anki의 영어 - 프랑스 번역 셋 이용\n",
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
        "!unzip -q fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 살펴보기\n",
        "text = 'fra.txt'\n",
        "with open(text) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "text_pairs = []\n",
        "for line in lines: # 라인별 처리\n",
        "    eng, fra = line.split('\\t')\n",
        "    fra = '[start]' + fra + '[end]'\n",
        "    text_pairs.append((eng, fra))\n",
        "    \n",
        "# 랜덤 문장보기    \n",
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hY_Feg5eKT",
        "outputId": "9ccbebbd-8dfe-4e5a-e554-c003dcdc806d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I'm extremely happy.\", '[start]Je suis extrêmement heureux.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 세트 분리"
      ],
      "metadata": {
        "id": "-r75ZlPc5lsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 셔플후 세트 분리\n",
        "random.shuffle(text_pairs)\n",
        "num_val_sam = int(0.15 * len(text_pairs))\n",
        "num_train_sam = len(text_pairs) - 2 * num_val_sam\n",
        "train_pairs = text_pairs[: num_train_sam]\n",
        "val_pairs = text_pairs[num_train_sam : num_train_sam + num_val_sam]\n",
        "test_pairs = text_pairs[num_train_sam + num_val_sam : ]"
      ],
      "metadata": {
        "id": "gnlxsWNL5eHT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TV층 (TextVectorization)준비\n",
        "- 영어층, 프랑스어층\n",
        "- 문자열 전처리 방식 커스텀 (fra_standard)"
      ],
      "metadata": {
        "id": "1dby35MF5pTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# 프랑스어 TV층에 적용하기 위해 특수 문자들 삭제\n",
        "str_chars = string.punctuation \n",
        "str_chars = str_chars.replace(\"[\",\"\")\n",
        "str_chars = str_chars.replace(\"]\", \"\")\n",
        "# 문자열 표준화 함수 정의\n",
        "def fra_standard(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "    lowercase, f\"[{re.escape(str_chars)}]\", \"\")\n",
        "\n",
        "# 범위제한을 위한 작업(예시라서 제한함)\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "#영어층\n",
        "source_vec = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length,\n",
        ")\n",
        "# french layer\n",
        "target_vec = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = fra_standard\n",
        ")\n",
        "\n",
        "# 훈련후 어휘 사전 만들기\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_fra_texts = [pair[1] for pair in train_pairs]\n",
        "source_vec.adapt(train_eng_texts)\n",
        "target_vec.adapt(train_fra_texts)"
      ],
      "metadata": {
        "id": "GC3QOmIe5d9L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 번역작업을 위한 데이터셋 준비"
      ],
      "metadata": {
        "id": "hQzjUJDc5wQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, fra):\n",
        "    eng = source_vec(eng)\n",
        "    fra = target_vec(fra)\n",
        "    return ({ # 이 dict가 inputs\n",
        "        'english': eng,\n",
        "        'french' : fra[:,:-1],\n",
        "    }, fra[:, 1:]) # fra가 target\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, fra_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    fra_texts = list(fra_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache() # 전처리 속도리를 높이기 위한 캐싱\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "h12jVtc150p8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크기 확인\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['eng'].shape : {inputs['english'].shape}\")\n",
        "  print(f\"inputs['fra'].shape : {inputs['french'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGymzRg5s68M",
        "outputId": "3465eebe-95dc-489e-e2c3-5d9902711fa5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['eng'].shape : (64, 20)\n",
            "inputs['fra'].shape : (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN 을 사용한 StoS모델\n",
        "- 시퀀스_투_시퀀스 모델"
      ],
      "metadata": {
        "id": "MRb5BnlL520_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# GRU기반 인코더\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "# 영어 소스 문장이 입력됨 (이름 지정시 입력 디셔너리로 모델 훈련 가능(키값))\n",
        "source = keras.Input(shape=(None,), dtype='int64', name= 'english')\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(source) # 마스킹 중요\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode = 'sum'\n",
        ")(x) # 양방향 GRU의 마지막 출력"
      ],
      "metadata": {
        "id": "BdEhhYQR548r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GRU기반 디코더와 엔드 투 엔드 모델"
      ],
      "metadata": {
        "id": "_jGDJxDdubbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_tar = keras.Input(shape=(None,), dtype = 'int64', name = 'french') # 타깃 시퀀스\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(past_tar)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences = True)\n",
        "x = decoder_gru(x, initial_state = encoded_source) # 디코더 GRU의 초기 상태\n",
        "x = layers.Dropout( 0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation = 'softmax')(x) # 다음 토큰 예측\n",
        "s2s_rnn = keras.Model([source, past_tar], target_next_step)\n",
        "# 엔투엔 모델은 소스 시퀀스와 타겟 시퀀스를 한 스템 앞의 타깃 시퀀스에 매핑"
      ],
      "metadata": {
        "id": "y-fa9hyFuZpb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rnn 기반 s2s모델 훈련\n",
        "s2s_rnn.compile(\n",
        "    optimizer = 'rmsprop',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "s2s_rnn.fit(train_ds, epochs = 15, validation_data = val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWnpp33xxkhs",
        "outputId": "3ceb3182-2a17-4d5c-8873-9825538f8e59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1828/1828 [==============================] - 183s 91ms/step - loss: 1.4823 - accuracy: 0.3569 - val_loss: 1.1825 - val_accuracy: 0.4678\n",
            "Epoch 2/15\n",
            "1828/1828 [==============================] - 160s 88ms/step - loss: 1.1892 - accuracy: 0.4844 - val_loss: 1.0401 - val_accuracy: 0.5363\n",
            "Epoch 3/15\n",
            "1828/1828 [==============================] - 170s 93ms/step - loss: 1.0638 - accuracy: 0.5374 - val_loss: 0.9829 - val_accuracy: 0.5667\n",
            "Epoch 4/15\n",
            "1828/1828 [==============================] - 170s 93ms/step - loss: 1.0081 - accuracy: 0.5696 - val_loss: 0.9720 - val_accuracy: 0.5807\n",
            "Epoch 5/15\n",
            "1828/1828 [==============================] - 171s 93ms/step - loss: 0.9845 - accuracy: 0.5913 - val_loss: 0.9670 - val_accuracy: 0.5901\n",
            "Epoch 6/15\n",
            "1828/1828 [==============================] - 170s 93ms/step - loss: 0.9695 - accuracy: 0.6073 - val_loss: 0.9677 - val_accuracy: 0.5941\n",
            "Epoch 7/15\n",
            "1828/1828 [==============================] - 168s 92ms/step - loss: 0.9596 - accuracy: 0.6178 - val_loss: 0.9703 - val_accuracy: 0.5971\n",
            "Epoch 8/15\n",
            "1828/1828 [==============================] - 161s 88ms/step - loss: 0.9529 - accuracy: 0.6254 - val_loss: 0.9712 - val_accuracy: 0.5996\n",
            "Epoch 9/15\n",
            "1828/1828 [==============================] - 158s 87ms/step - loss: 0.9486 - accuracy: 0.6306 - val_loss: 0.9734 - val_accuracy: 0.6006\n",
            "Epoch 10/15\n",
            "1828/1828 [==============================] - 158s 86ms/step - loss: 0.9460 - accuracy: 0.6334 - val_loss: 0.9767 - val_accuracy: 0.6008\n",
            "Epoch 11/15\n",
            "1828/1828 [==============================] - 160s 87ms/step - loss: 0.9450 - accuracy: 0.6353 - val_loss: 0.9787 - val_accuracy: 0.6002\n",
            "Epoch 12/15\n",
            "1828/1828 [==============================] - 159s 87ms/step - loss: 0.9459 - accuracy: 0.6357 - val_loss: 0.9816 - val_accuracy: 0.5992\n",
            "Epoch 13/15\n",
            "1828/1828 [==============================] - 167s 91ms/step - loss: 0.9478 - accuracy: 0.6352 - val_loss: 0.9829 - val_accuracy: 0.6007\n",
            "Epoch 14/15\n",
            "1828/1828 [==============================] - 170s 93ms/step - loss: 0.9513 - accuracy: 0.6331 - val_loss: 0.9872 - val_accuracy: 0.5980\n",
            "Epoch 15/15\n",
            "1828/1828 [==============================] - 164s 90ms/step - loss: 0.9550 - accuracy: 0.6315 - val_loss: 0.9895 - val_accuracy: 0.5964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f169c662d30>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN인코더와 디코더로 새로운 문장 번역"
      ],
      "metadata": {
        "id": "SHsJFzJWyDw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 예측된 인덱스를 문자열 ㅇ토큰으로 변환(딕셔너리로)\n",
        "fra_vocab = target_vec.get_vocabulary()\n",
        "fra_idx_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "# 함수정의\n",
        "def decode_sequence(input_sen):\n",
        "  tokenized_input_sen = source_vec([input_sen])\n",
        "  decoded_sen = '[start]'\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sen = target_vec([decoded_sen])\n",
        "    next_token_preds =s2s_rnn.predict(\n",
        "        [tokenized_input_sen, tokenized_target_sen]\n",
        "    ) \n",
        "    sampled_token_idx = np.argmax(next_token_preds[0, i, :])  # 다음 토큰 샘플링\n",
        "    sampled_token = fra_idx_lookup[sampled_token_idx] # 예측된 토큰을 문자열로\n",
        "    decoded_sen += \" \" + sampled_token # 생성된 문장 추가\n",
        "    if sampled_token == \"[end]\":     # 종료 조건 (최대길이 도달 또는 end)\n",
        "      break\n",
        "  return decoded_sen\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in text_pairs]\n",
        "for _ in range(20):\n",
        "  input_sen = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sen)\n",
        "  print(decode_sequence(input_sen))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr-HOkpZyCLa",
        "outputId": "96be18c1-7fe2-4b42-b1dd-0029c31207ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Our fighters averaged 430 missions a day.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] [UNK] a une [UNK] de [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I had fun.\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[start] me suis [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I ran out of the house.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[start] [UNK] de la maison[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We all agree with you.\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] nous sommes tous [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom is almost ready to go.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[start] est presque prêt à y aller[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "We all knew it.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] le [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "I never saw Tom again.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[start] na jamais vu tom[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "What if something went wrong?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] quelque chose [UNK] quelque chose de [UNK] [end]\n",
            "-\n",
            "My husband is the jealous type.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] est le plus [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "It's a vague story.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] une [UNK] [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "She couldn't hold back her laughter.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[start] ne pouvait pas ses [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "Tom told Mary not to swim with John.\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] a dit à mary de ne pas venir avec mary[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "He's an interpreter in an international bank.\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] un une dans la [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "My wife and I did our Christmas shopping in October.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] et moi avons fait de faire des choses de notre ne se [UNK] dans une de nombreuses jours de ne\n",
            "-\n",
            "Don't trust anyone.\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] ne te [UNK] à qui que ce soit [end]\n",
            "-\n",
            "Tom called me fat.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[start] ma appelé aussi [end]\n",
            "-\n",
            "I'll come back for you.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[start] vous le [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "He must be homesick.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] doit le mal de le faire[end]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "My new job is harder than my old one.\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[start] ma nouvelle plus heureux que mon père est de la [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "-\n",
            "What time does the store open?\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] temps le temps [UNK]  [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  트랜스 포머를 사용한 s2s모델\n",
        "- 트랜스포머 디코더 클래스 정의"
      ],
      "metadata": {
        "id": "3xNVaLl72l-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim # 입력 토큰 벡터의 크기 \n",
        "    self.dense_dim = dense_dim # 내부 밀집 층의 크기\n",
        "    self.num_heads = num_heads # 어텐션 해드 개수\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim = embed_dim\n",
        "    )\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim = embed_dim\n",
        "    )\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation = 'relu'),\n",
        "         layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "# 모델 저장을 위한 직렬화(직렬형태여애 저장이 가능)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        'embed_dim' : self.embed_dim,\n",
        "        'num_heads' : self.num_heads,\n",
        "        'dense_dim' : self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "# 코잘마스킹을 생성하는 함수  (코잘 패딩을고려하여 어텐션 층 전달을 위한 행렬을 만듦)\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis] \n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype ='int32') # 절반은 1이고, 나머지는 0인  행렬\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "# 정방향 패스를 구현하는 완전한 연산함수?\n",
        "  def call(self, inputs, encoder_outputs, mask = None): \n",
        "    causal_mask = self.get_causal_attention_mask(inputs) # 코잘 마스킹 추출\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(\n",
        "          mask[:,tf.newaxis, :], dtype ='int32'\n",
        "      )\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask) # 두 마스킹을 함침\n",
        "      attention_output_1 = self.attention_1(\n",
        "          query = inputs,\n",
        "          value = inputs,\n",
        "          key = inputs,\n",
        "          attention_mask = causal_mask\n",
        "      ) # 코잘 마스킹을 타깃 시퀀스에 대해 셀프 어텐션을 수행하는 첫번째 어텐션 층으로 전달\n",
        "      attention_output_1 = self.layernorm_1(inputs + attention_output_1) \n",
        "      attention_output_2 = self.attention_2(\n",
        "          query = attention_output_1,\n",
        "          value = encoder_outputs,\n",
        "          key = encoder_outputs,\n",
        "          attention_mask = padding_mask\n",
        "      ) # 마스킹 소시 시퀀스와 타깃 시퀀스를 연관 시키는 두번째 어텐션 층\n",
        "      attention_output_2 = self.layernorm_2(\n",
        "        attention_output_1 + attention_output_2)\n",
        "      proj_output = self.dense_proj(attention_output_2)\n",
        "      return self.layernorm_3(attention_output_2 + proj_output) \n"
      ],
      "metadata": {
        "id": "OFhZkCHy2v_Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기계번역을 위한 트랜스포머"
      ],
      "metadata": {
        "id": "0IDW4TnzFZax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위치 임베딩 층\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "L4-yjhC8FX3P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 엔드투엔드 트랜스포머\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "cCBvFMd9FjG1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # 소스 문장 인코딩\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "# 타깃 시퀀스를 인코딩하고 인코딩된 소스 문장과 합침\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) # 출력위치의 단어 예측\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "gSIiPbutFpoT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s2s트랜스포머 훈련\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64gv4aCRFtbD",
        "outputId": "274e772f-ca9d-4544-99ce-f51d886794d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1828/1828 [==============================] - 141s 75ms/step - loss: 1.6059 - accuracy: 0.3144 - val_loss: 1.2888 - val_accuracy: 0.4180\n",
            "Epoch 2/30\n",
            "1828/1828 [==============================] - 136s 74ms/step - loss: 1.2888 - accuracy: 0.4439 - val_loss: 1.1192 - val_accuracy: 0.4966\n",
            "Epoch 3/30\n",
            "1828/1828 [==============================] - 135s 74ms/step - loss: 1.1454 - accuracy: 0.5044 - val_loss: 1.0513 - val_accuracy: 0.5346\n",
            "Epoch 4/30\n",
            "1828/1828 [==============================] - 137s 75ms/step - loss: 1.0793 - accuracy: 0.5390 - val_loss: 1.0241 - val_accuracy: 0.5528\n",
            "Epoch 5/30\n",
            "1828/1828 [==============================] - 131s 72ms/step - loss: 1.0513 - accuracy: 0.5602 - val_loss: 1.0140 - val_accuracy: 0.5627\n",
            "Epoch 6/30\n",
            "1828/1828 [==============================] - 129s 70ms/step - loss: 1.0331 - accuracy: 0.5747 - val_loss: 1.0094 - val_accuracy: 0.5671\n",
            "Epoch 7/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 1.0183 - accuracy: 0.5868 - val_loss: 1.0029 - val_accuracy: 0.5747\n",
            "Epoch 8/30\n",
            "1828/1828 [==============================] - 129s 71ms/step - loss: 1.0047 - accuracy: 0.5969 - val_loss: 0.9981 - val_accuracy: 0.5790\n",
            "Epoch 9/30\n",
            "1828/1828 [==============================] - 129s 70ms/step - loss: 0.9909 - accuracy: 0.6059 - val_loss: 0.9938 - val_accuracy: 0.5830\n",
            "Epoch 10/30\n",
            "1828/1828 [==============================] - 129s 71ms/step - loss: 0.9794 - accuracy: 0.6132 - val_loss: 0.9902 - val_accuracy: 0.5871\n",
            "Epoch 11/30\n",
            "1828/1828 [==============================] - 129s 71ms/step - loss: 0.9659 - accuracy: 0.6211 - val_loss: 0.9948 - val_accuracy: 0.5866\n",
            "Epoch 12/30\n",
            "1828/1828 [==============================] - 129s 71ms/step - loss: 0.9543 - accuracy: 0.6269 - val_loss: 0.9834 - val_accuracy: 0.5916\n",
            "Epoch 13/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.9413 - accuracy: 0.6330 - val_loss: 0.9840 - val_accuracy: 0.5935\n",
            "Epoch 14/30\n",
            "1828/1828 [==============================] - 129s 71ms/step - loss: 0.9321 - accuracy: 0.6383 - val_loss: 0.9813 - val_accuracy: 0.5975\n",
            "Epoch 15/30\n",
            "1828/1828 [==============================] - 129s 70ms/step - loss: 0.9204 - accuracy: 0.6438 - val_loss: 0.9813 - val_accuracy: 0.5986\n",
            "Epoch 16/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.9092 - accuracy: 0.6489 - val_loss: 0.9783 - val_accuracy: 0.6000\n",
            "Epoch 17/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.8992 - accuracy: 0.6535 - val_loss: 0.9768 - val_accuracy: 0.6012\n",
            "Epoch 18/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8921 - accuracy: 0.6578 - val_loss: 0.9852 - val_accuracy: 0.6016\n",
            "Epoch 19/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.8825 - accuracy: 0.6626 - val_loss: 0.9853 - val_accuracy: 0.6015\n",
            "Epoch 20/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8731 - accuracy: 0.6661 - val_loss: 0.9854 - val_accuracy: 0.6021\n",
            "Epoch 21/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8637 - accuracy: 0.6699 - val_loss: 0.9804 - val_accuracy: 0.6030\n",
            "Epoch 22/30\n",
            "1828/1828 [==============================] - 127s 69ms/step - loss: 0.8575 - accuracy: 0.6739 - val_loss: 0.9884 - val_accuracy: 0.6047\n",
            "Epoch 23/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.8517 - accuracy: 0.6767 - val_loss: 0.9899 - val_accuracy: 0.6043\n",
            "Epoch 24/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.8439 - accuracy: 0.6802 - val_loss: 0.9851 - val_accuracy: 0.6071\n",
            "Epoch 25/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8358 - accuracy: 0.6834 - val_loss: 0.9883 - val_accuracy: 0.6069\n",
            "Epoch 26/30\n",
            "1828/1828 [==============================] - 127s 69ms/step - loss: 0.8283 - accuracy: 0.6864 - val_loss: 0.9922 - val_accuracy: 0.6034\n",
            "Epoch 27/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8215 - accuracy: 0.6893 - val_loss: 0.9915 - val_accuracy: 0.6069\n",
            "Epoch 28/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8188 - accuracy: 0.6917 - val_loss: 0.9952 - val_accuracy: 0.6078\n",
            "Epoch 29/30\n",
            "1828/1828 [==============================] - 127s 70ms/step - loss: 0.8128 - accuracy: 0.6939 - val_loss: 0.9950 - val_accuracy: 0.6087\n",
            "Epoch 30/30\n",
            "1828/1828 [==============================] - 128s 70ms/step - loss: 0.8061 - accuracy: 0.6968 - val_loss: 1.0020 - val_accuracy: 0.6087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16867836a0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 모델을 사용한 문장번역(시투시)"
      ],
      "metadata": {
        "id": "WwcC-AZTHz9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "fra_vocab = target_vec.get_vocabulary()\n",
        "fra_idx_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sen(input_sen):\n",
        "  tokenized_input_sen = source_vec([input_sen])\n",
        "  decoded_sen = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sen = target_vec(\n",
        "        [decoded_sen])[:,:-1]\n",
        "    preds = transformer(\n",
        "        [tokenized_input_sen, tokenized_target_sen]\n",
        "    )\n",
        "    sampled_token_idx = np.argmax(preds[0,i,:]) # 다음 토큰을 샘플링\n",
        "    sampled_token = fra_idx_lookup[sampled_token_idx]\n",
        "    decoded_sen += \" \" + sampled_token\n",
        "    if sampled_token == '[end]':\n",
        "      break\n",
        "  return decoded_sen\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sen = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sen)\n",
        "  print(decode_sen(input_sen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J0hVTyKHqrH",
        "outputId": "b1359915-51b1-4a1f-aaaf-6e2109ea1f0d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Are you pleased with your new job?\n",
            "[start] êtesvous ravi votre nouveau [UNK]  [end]\n",
            "-\n",
            "I almost left my umbrella in the train.\n",
            "[start] presque mon [UNK] dans le train[end]     dans      dans  [end]\n",
            "-\n",
            "I just don't want to hurt anyone.\n",
            "[start] tout simplement pas faire mal à personne[end]       à   à   à\n",
            "-\n",
            "I can't tell you the truth.\n",
            "[start] ne pouvezvous pas te dire la vérité[end]             \n",
            "-\n",
            "I have to stay for a while.\n",
            "[start] devraisje rester un moment[end]             [end]\n",
            "-\n",
            "Do I look presentable?\n",
            "[start] [UNK] [UNK]  [end]\n",
            "-\n",
            "I thought that was a great story.\n",
            "[start] [UNK] que cétait une belle histoire [UNK]             \n",
            "-\n",
            "Tom gave this apple to me.\n",
            "[start] me [UNK] cette carte  pour [end]\n",
            "-\n",
            "Don't make me angry.\n",
            "[start] le [UNK]  en [end]\n",
            "-\n",
            "People are complicated.\n",
            "[start] ont des gens[end]              [end]\n",
            "-\n",
            "I'd say you did well.\n",
            "[start] le fait pas bien[end]          [end]\n",
            "-\n",
            "You're very clever.\n",
            "[start] sommes très [UNK]              [end]\n",
            "-\n",
            "I don't even know where.\n",
            "[start] ne savent même pas où aller[end]        [end]\n",
            "-\n",
            "It is cold there, even in summer.\n",
            "[start] [UNK] il y a davantage [UNK]  en en en [end]\n",
            "-\n",
            "The water turned to ice.\n",
            "[start] ont étudié de la mauvaise [UNK]  de [end]\n",
            "-\n",
            "I want to spend my life with you.\n",
            "[start] voulons passer ma vie avec vous[end]    pour avec de avec [end]\n",
            "-\n",
            "Tom should never have borrowed money from Mary.\n",
            "[start] aurait peutêtre jamais bien [UNK] de largent[end]   de de à à de mary[end]     \n",
            "-\n",
            "You almost died.\n",
            "[start] se trouve presque [UNK]       [end]\n",
            "-\n",
            "I am just going for a walk.\n",
            "[start] nous [UNK] pour faire une france[end]   [end]\n",
            "-\n",
            "We don't have time to do that today.\n",
            "[start] nen ont pas le temps aujourdhui[end]    aujourdhui[end]       aujourdhui[end] aujourdhui[end]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5eD7yiFdd-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhnO4t3YYL0wHeTENVSDaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhdixhfl/last_project/blob/main/%ED%95%9C_%EC%98%81_%EB%B2%88%EC%97%AD%EA%B8%B0_sequence_to_sequence_transformerModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhVrDgzefQa1",
        "outputId": "a5e4ffa4-c609-4a73-a884-9ce052ead45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-06 05:25:54--  https://www.manythings.org/anki/kor-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153571 (150K) [application/zip]\n",
            "Saving to: ‘kor-eng.zip’\n",
            "\n",
            "kor-eng.zip         100%[===================>] 149.97K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-01-06 05:25:55 (1009 KB/s) - ‘kor-eng.zip’ saved [153571/153571]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## 데이터 셋 로딩\n",
        "# https://www.manythings.org/anki의 영어 - 프랑스 번역 셋 이용\n",
        "!wget https://www.manythings.org/anki/kor-eng.zip\n",
        "!unzip -q kor-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 살펴보기\n",
        "text = 'kor.txt'\n",
        "with open(text) as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "text_pairs = []\n",
        "for line in lines: # 라인별 처리\n",
        "    eng, kor, etc = line.split('\\t')\n",
        "    eng = '[start]' + eng + '[end]'\n",
        "    text_pairs.append((kor, eng))\n",
        "    \n",
        "# 랜덤 문장보기    \n",
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nouttwGyf4N0",
        "outputId": "889d0011-6856-49c6-df4f-12a50de8e044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('톰, 메리, 존, 앨리스는 모두 수영을 할 수 있어.', '[start]Tom, Mary, John and Alice can all swim.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 셔플후 세트 분리\n",
        "random.shuffle(text_pairs)\n",
        "num_val_sam = int(0.15 * len(text_pairs))\n",
        "num_train_sam = len(text_pairs) - 2 * num_val_sam\n",
        "train_pairs = text_pairs[: num_train_sam]\n",
        "val_pairs = text_pairs[num_train_sam : num_train_sam + num_val_sam]\n",
        "test_pairs = text_pairs[num_train_sam + num_val_sam : ]"
      ],
      "metadata": {
        "id": "jFAzCZ2zf4Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# 프랑스어 TV층에 적용하기 위해 특수 문자들 삭제\n",
        "str_chars = string.punctuation \n",
        "str_chars = str_chars.replace(\"[\",\"\")\n",
        "str_chars = str_chars.replace(\"]\", \"\")\n",
        "# 문자열 표준화 함수 정의\n",
        "def eng_standard(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "    lowercase, f\"[{re.escape(str_chars)}]\", \"\")\n",
        "\n",
        "# 범위제한을 위한 작업(예시라서 제한함)\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# 한글\n",
        "source_vec = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length,\n",
        ")\n",
        "# 영어\n",
        "target_vec = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = eng_standard\n",
        ")\n",
        "\n",
        "# 훈련후 어휘 사전 만들기\n",
        "train_kor_texts = [pair[0] for pair in train_pairs]\n",
        "train_eng_texts = [pair[1] for pair in train_pairs]\n",
        "source_vec.adapt(train_kor_texts)\n",
        "target_vec.adapt(train_eng_texts)"
      ],
      "metadata": {
        "id": "9UzVNGtZf4JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(kor, eng):\n",
        "    kor = source_vec(kor)\n",
        "    eng = target_vec(eng)\n",
        "    return ({ # 이 dict가 inputs\n",
        "        'korea': kor,\n",
        "        'english' : eng[:,:-1],\n",
        "    }, eng[:, 1:]) # eng가 target\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    kor_texts, eng_texts = zip(*pairs)\n",
        "    kor_texts = list(kor_texts)\n",
        "    eng_texts = list(eng_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((kor_texts, eng_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache() # 전처리 속도리를 높이기 위한 캐싱\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "MW6zcvbOf4G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "Kpe5GUWVj9sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 크기 확인\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['kor'].shape : {inputs['korea'].shape}\")\n",
        "  print(f\"inputs['eng'].shape : {inputs['english'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sET4jJXwf4EU",
        "outputId": "aa3a55f5-e432-4d7e-9be8-6cd2752a418f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['kor'].shape : (11, 20)\n",
            "inputs['eng'].shape : (11, 20)\n",
            "targets.shape: (11, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim # 입력 토큰 벡터의 크기 \n",
        "    self.dense_dim = dense_dim # 내부 밀집 층의 크기\n",
        "    self.num_heads = num_heads # 어텐션 해드 개수\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim = embed_dim\n",
        "    )\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim = embed_dim\n",
        "    )\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation = 'relu'),\n",
        "         layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "# 모델 저장을 위한 직렬화(직렬형태여애 저장이 가능)\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        'embed_dim' : self.embed_dim,\n",
        "        'num_heads' : self.num_heads,\n",
        "        'dense_dim' : self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "# 코잘마스킹을 생성하는 함수  (코잘 패딩을고려하여 어텐션 층 전달을 위한 행렬을 만듦)\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis] \n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype ='int32') # 절반은 1이고, 나머지는 0인  행렬\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "# 정방향 패스를 구현하는 완전한 연산함수?\n",
        "  def call(self, inputs, encoder_outputs, mask = None): \n",
        "    causal_mask = self.get_causal_attention_mask(inputs) # 코잘 마스킹 추출\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(\n",
        "          mask[:,tf.newaxis, :], dtype ='int32'\n",
        "      )\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask) # 두 마스킹을 함침\n",
        "      attention_output_1 = self.attention_1(\n",
        "          query = inputs,\n",
        "          value = inputs,\n",
        "          key = inputs,\n",
        "          attention_mask = causal_mask\n",
        "      ) # 코잘 마스킹을 타깃 시퀀스에 대해 셀프 어텐션을 수행하는 첫번째 어텐션 층으로 전달\n",
        "      attention_output_1 = self.layernorm_1(inputs + attention_output_1) \n",
        "      attention_output_2 = self.attention_2(\n",
        "          query = attention_output_1,\n",
        "          value = encoder_outputs,\n",
        "          key = encoder_outputs,\n",
        "          attention_mask = padding_mask\n",
        "      ) # 마스킹 소시 시퀀스와 타깃 시퀀스를 연관 시키는 두번째 어텐션 층\n",
        "      attention_output_2 = self.layernorm_2(\n",
        "        attention_output_1 + attention_output_2)\n",
        "      proj_output = self.dense_proj(attention_output_2)\n",
        "      return self.layernorm_3(attention_output_2 + proj_output) "
      ],
      "metadata": {
        "id": "tmzrIrP7f4B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위치 임베딩 층\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "D9495Pfgf3-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 엔드투엔드 트랜스포머\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "DmDuyf_8hT9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "gUd6QRJ7k5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"korea\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # 소스 문장 인코딩\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "# 타깃 시퀀스를 인코딩하고 인코딩된 소스 문장과 합침\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) # 출력위치의 단어 예측\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "pMi_PnmwhT7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s2s트랜스포머 훈련\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)\n",
        "# print(f'정확도 : {transformer.evaluate(test_ds)[1]:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7rkJomn8hT4z",
        "outputId": "7d7f81c9-0da0-4fa2-8530-16811508de00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "43/43 [==============================] - 11s 83ms/step - loss: 1.8114 - accuracy: 0.1774 - val_loss: 1.6635 - val_accuracy: 0.1527\n",
            "Epoch 2/30\n",
            "43/43 [==============================] - 3s 72ms/step - loss: 1.5909 - accuracy: 0.2041 - val_loss: 1.5961 - val_accuracy: 0.2131\n",
            "Epoch 3/30\n",
            "43/43 [==============================] - 3s 68ms/step - loss: 1.5094 - accuracy: 0.2231 - val_loss: 1.5817 - val_accuracy: 0.2144\n",
            "Epoch 4/30\n",
            "43/43 [==============================] - 3s 68ms/step - loss: 1.4129 - accuracy: 0.2536 - val_loss: 1.5515 - val_accuracy: 0.2529\n",
            "Epoch 5/30\n",
            "43/43 [==============================] - 3s 72ms/step - loss: 1.3290 - accuracy: 0.2754 - val_loss: 1.5577 - val_accuracy: 0.2475\n",
            "Epoch 6/30\n",
            "43/43 [==============================] - 3s 68ms/step - loss: 1.2507 - accuracy: 0.2996 - val_loss: 1.4716 - val_accuracy: 0.2453\n",
            "Epoch 7/30\n",
            "43/43 [==============================] - 3s 68ms/step - loss: 1.1733 - accuracy: 0.3252 - val_loss: 1.4492 - val_accuracy: 0.2558\n",
            "Epoch 8/30\n",
            "43/43 [==============================] - 3s 68ms/step - loss: 1.1027 - accuracy: 0.3513 - val_loss: 1.4584 - val_accuracy: 0.2662\n",
            "Epoch 9/30\n",
            "43/43 [==============================] - 3s 78ms/step - loss: 1.0351 - accuracy: 0.3828 - val_loss: 1.4118 - val_accuracy: 0.2883\n",
            "Epoch 10/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.9689 - accuracy: 0.4132 - val_loss: 1.4113 - val_accuracy: 0.2842\n",
            "Epoch 11/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.8991 - accuracy: 0.4470 - val_loss: 1.4373 - val_accuracy: 0.2817\n",
            "Epoch 12/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.8204 - accuracy: 0.4925 - val_loss: 1.4543 - val_accuracy: 0.2845\n",
            "Epoch 13/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.7546 - accuracy: 0.5273 - val_loss: 1.4328 - val_accuracy: 0.2912\n",
            "Epoch 14/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.6957 - accuracy: 0.5671 - val_loss: 1.4731 - val_accuracy: 0.2921\n",
            "Epoch 15/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.6230 - accuracy: 0.6016 - val_loss: 1.4438 - val_accuracy: 0.3016\n",
            "Epoch 16/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.5661 - accuracy: 0.6406 - val_loss: 1.4650 - val_accuracy: 0.3019\n",
            "Epoch 17/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.4965 - accuracy: 0.6910 - val_loss: 1.5157 - val_accuracy: 0.2940\n",
            "Epoch 18/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.4480 - accuracy: 0.7260 - val_loss: 1.4765 - val_accuracy: 0.3060\n",
            "Epoch 19/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.3955 - accuracy: 0.7596 - val_loss: 1.4913 - val_accuracy: 0.3101\n",
            "Epoch 20/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.3481 - accuracy: 0.7949 - val_loss: 1.4981 - val_accuracy: 0.3165\n",
            "Epoch 21/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.3043 - accuracy: 0.8232 - val_loss: 1.5028 - val_accuracy: 0.3111\n",
            "Epoch 22/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.2623 - accuracy: 0.8544 - val_loss: 1.6227 - val_accuracy: 0.3177\n",
            "Epoch 23/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.2358 - accuracy: 0.8689 - val_loss: 1.5344 - val_accuracy: 0.3098\n",
            "Epoch 24/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.2013 - accuracy: 0.8915 - val_loss: 1.5893 - val_accuracy: 0.3111\n",
            "Epoch 25/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.1816 - accuracy: 0.8997 - val_loss: 1.5433 - val_accuracy: 0.3143\n",
            "Epoch 26/30\n",
            "43/43 [==============================] - 3s 70ms/step - loss: 0.1604 - accuracy: 0.9135 - val_loss: 1.5929 - val_accuracy: 0.3114\n",
            "Epoch 27/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.1445 - accuracy: 0.9233 - val_loss: 1.5620 - val_accuracy: 0.3215\n",
            "Epoch 28/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.1220 - accuracy: 0.9363 - val_loss: 1.5773 - val_accuracy: 0.3304\n",
            "Epoch 29/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.1100 - accuracy: 0.9435 - val_loss: 1.5652 - val_accuracy: 0.3231\n",
            "Epoch 30/30\n",
            "43/43 [==============================] - 3s 69ms/step - loss: 0.1034 - accuracy: 0.9460 - val_loss: 1.6156 - val_accuracy: 0.3073\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f39824c498a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics=[\"accuracy\"])\n\u001b[1;32m      6\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'정확도 : {transformer.evaluate(test_ds)[1]:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "eng_vocab = target_vec.get_vocabulary()\n",
        "eng_idx_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sen(input_sen):\n",
        "  tokenized_input_sen = source_vec([input_sen])\n",
        "  decoded_sen = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sen = target_vec(\n",
        "        [decoded_sen])[:,:-1]\n",
        "    preds = transformer(\n",
        "        [tokenized_input_sen, tokenized_target_sen]\n",
        "    )\n",
        "    sampled_token_idx = np.argmax(preds[0,i,:]) # 다음 토큰을 샘플링\n",
        "    sampled_token = eng_idx_lookup[sampled_token_idx]\n",
        "    decoded_sen += \" \" + sampled_token\n",
        "    if sampled_token == '[end]':\n",
        "      break\n",
        "  return decoded_sen\n",
        "\n",
        "test_kor_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sen = random.choice(test_kor_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sen)\n",
        "  print(decode_sen(input_sen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKlYhM3EhT2T",
        "outputId": "28e59898-2e9d-4ef5-c43c-cb4f235633aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "톰은 예민한 것 같아.\n",
            "[start] think tom is lonely[end]                his\n",
            "-\n",
            "한번이라도 도둑맞았던 적이 있어?\n",
            "[start] have no social skills[end]              probably  when\n",
            "-\n",
            "뭔가 먹어.\n",
            "[start] have an old country[end]            and they probably  something\n",
            "-\n",
            "우린 서로 얘기했어.\n",
            "[start] learned how to cook from tom[end]              \n",
            "-\n",
            "톰이 다시 보스턴으로 돌아올 줄은 생각지도 못했다.\n",
            "[start] asked tom to take back his birthday party[end]     interested[end]       when\n",
            "-\n",
            "그녀는 발이 작다.\n",
            "[start] used a little dangerous[end]            and me probably  when\n",
            "-\n",
            "뭐 하러?\n",
            "[start] asked tom to play the guitar[end]            probably  when\n",
            "-\n",
            "톰이 자신이 왜 떠나야 하는지를 이해했으면 좋겠어.\n",
            "[start] asked tom a few of a red house[end]         so probably  when\n",
            "-\n",
            "내가 널 어디선가 본 적 있지 않았어?\n",
            "[start] i didnt know how my more food[end]      native   and they probably  when\n",
            "-\n",
            "그녀는 꽃을 좋아한다고 말했다.\n",
            "[start] asked tom a few questions that science teacher[end]          probably  when\n",
            "-\n",
            "그 사람들은 도망 쳤어.\n",
            "[start] medicine tastes bitter[end]          lied[end]       and\n",
            "-\n",
            "이거 시도해봐.\n",
            "[start] this quite similar[end]             time[end]    \n",
            "-\n",
            "톰이 뒷마당에 야자 나무를 심고 있다.\n",
            "[start] asked tom to a few are going on[end]     job[end]     probably  when\n",
            "-\n",
            "게임 한판 하자.\n",
            "[start] used a few questions[end]            and his probably  when\n",
            "-\n",
            "톰은 프랑스어를 배우러 여기에 왔어.\n",
            "[start] used to learn french[end]             his   when\n",
            "-\n",
            "못 믿겠는데.\n",
            "[start] so glad that youre here[end]               \n",
            "-\n",
            "겨울 방학은 언제 시작하나요?\n",
            "[start] learned how a few questions[end]             probably  when\n",
            "-\n",
            "나는 몰라요.\n",
            "[start] used a mary child[end]              dog[end]  \n",
            "-\n",
            "이걸 끝내.\n",
            "[start] your favorite tennis player[end]              probably  when\n",
            "-\n",
            "나는 살빼는 방법을 알고 싶다.\n",
            "[start] do that my best[end]         his       when\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXqcsOCphTxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}